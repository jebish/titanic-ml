# ğŸš€ Titanic Survival Prediction - Machine Learning Pipeline

A complete, modular, and scalable ML system to predict passenger survival on the Titanic using Logistic Regression, Random Forest, and a Neural Network.

- âœ… Modular Code
- âœ… Training, Evaluation, and Inference
- âœ… Results Saving

---

## ğŸ“‚ Project Structure

```
titanic-ml-project/
â”‚-- src/                  # Source code
â”‚   â”‚-- __init__.py       
â”‚   â”‚-- data_loader.py    # Handles data loading & preprocessing
â”‚   â”‚-- train.py          # Trains models & saves them
â”‚   â”‚-- evaluate.py       # Evaluates models & saves reports
â”‚   â”‚-- inference.py      # Runs inference using trained models
â”‚   
â”‚-- main.py               # Runs training, evaluation & inference together
â”‚-- notebooks/            # Jupyter notebooks for the trial of Neural Network (optional)
â”‚-- models/               # Stores trained models
â”‚-- results/              # Stores evaluation reports (text/images)
â”‚-- data/                 # Directory for dataset (not included)
â”‚-- requirements.txt      # Python dependencies
â”‚-- README.md             # Project documentation
```
---

## ğŸ“Š Dataset

The dataset used is the Titanic Survival Dataset from [Kaggle - Titanic Dataset](https://www.kaggle.com/datasets/yasserh/titanic-dataset/data).

It contains 12 features, including whether the passenger survived or not.  
However, some of the features aren't used, and the data contains null values. All the preprocessing is done by `data_loader.py`.

### Features Used

| Feature       | Description                          |
|---------------|--------------------------------------|
| Pclass        | Ticket Class                         |
| Sex           | Gender                               |
| Age           | Age of the passenger                 |
| SibSp         | Number of siblings/spouses aboard    |
| Parch         | Number of parents/children aboard    |
| Fare          | Ticket price                         |
| Embarked      | Port of embarkation                  |

ğŸ”¸ **Target Variable**: Survived (0 = No, 1 = Yes)

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/titanic-ml-project.git
cd titanic-ml-project
```

### 2ï¸âƒ£ Create a Virtual Environment (Optional)

```bash
python -m venv venv
venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Download the Dataset

Download & Extract the [dataset](https://www.kaggle.com/datasets/yasserh/titanic-dataset/data) and place the ".csv" file inside the data/ folder.

Alternatively, you can provide a dataset path as an argument.

<h1>ğŸš€ Running the Project</h1>

### 1ï¸âƒ£ Train Models

```bash
python -m src.train
```

### 2ï¸âƒ£ Evaluate Models

```bash
python -m src.evaluate
```

### 3ï¸âƒ£ Perform Inference (User Input)

```bash
python -m src.inference
```

## ğŸ”¥ Perform Train, Evaluation, and Inference at once

```bash
python main.py   #user data/Titanic-Dataset.csv as data
python main.py --data "your_data_location" #Provide another location for dataset
```

## ğŸ“ˆ Model Performance

### === Logistic Regression ===
```
              precision    recall  f1-score   support
           0       0.88      0.81      0.84       105
           1       0.76      0.84      0.79        74

    accuracy                           0.82       179
   macro avg       0.82      0.82      0.82       179
weighted avg       0.83      0.82      0.82       179
```

### === Random Forest Model ===
```
              precision    recall  f1-score   support
           0       0.83      0.87      0.85       105
           1       0.80      0.74      0.77        74

    accuracy                           0.82       179
   macro avg       0.81      0.80      0.81       179
weighted avg       0.81      0.82      0.81       179
```

### === Neural Network Model ===
```
              precision    recall  f1-score   support
           0       0.80      0.84      0.82       105
           1       0.75      0.70      0.73        74

    accuracy                           0.78       179
   macro avg       0.78      0.77      0.77       179
weighted avg       0.78      0.78      0.78       179
```

##ğŸ’¡Future Improvements

âŒ› Hyperparameter tuning (Grid Search / Bayesian Optimization)
âŒ› More advanced models (XGBoost, SVM)
âŒ› Deployment (Flask API, Streamlit, or FastAPI)

<h1>ğŸ“By Jebish7</h1>
